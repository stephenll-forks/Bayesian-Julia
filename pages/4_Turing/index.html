<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/Bayesian-Julia/libs/katex/katex.min.css"> <link rel=stylesheet  href="/Bayesian-Julia/libs/highlight/github.min.css"> <link rel=stylesheet  href="/Bayesian-Julia/css/jtd.css"> <link rel=icon  href="/Bayesian-Julia/assets/favicon.ico"> <title>How to use Turing</title> <div class=page-wrap > <div class=side-bar > <div class=header > <a href="/Bayesian-Julia/" class=title > Bayesian Stats </a> </div> <label for=show-menu  class=show-menu >MENU</label> <input type=checkbox  id=show-menu  role=button > <div class=menu  id=side-menu > <ul class=menu-list > <li class="menu-list-item "><a href="/Bayesian-Julia/" class="menu-list-link ">Home</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/1_why_Julia/" class="menu-list-link ">1. Why Julia?</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/2_bayes_stats/" class="menu-list-link ">2. What is Bayesian Statistics?</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/3_prob_dist/" class="menu-list-link ">3. Common Probability Distributions</a> <li class="menu-list-item active"><a href="/Bayesian-Julia/pages/4_Turing/" class="menu-list-link active">4. How to use Turing</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/5_MCMC/" class="menu-list-link ">5. Markov Chain Monte Carlo (MCMC)</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/6_linear_reg/" class="menu-list-link ">6. Bayesian Linear Regression</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/7_logistic_reg/" class="menu-list-link ">7. Bayesian Logistic Regression</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/8_count_reg/" class="menu-list-link ">8. Bayesian Regression with Count Data</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/9_robust_reg/" class="menu-list-link ">9. Robust Bayesian Regression</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/10_multilevel_models/" class="menu-list-link ">10. Multilevel Models</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/11_Turing_tricks/" class="menu-list-link ">11. Computational Tricks with Turing</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/12_epi_models/" class="menu-list-link ">12. Bayesian Epidemiological Models</a> </ul> </div> <div class=footer > <a href="https://www.julialang.org"><img style="height:50px;padding-left:10px;margin-bottom:15px;" src="https://julialang.org/assets/infra/logo.svg" alt="Julia Logo"></a> </div> </div> <div class=main-content-wrap > <div class=main-content > <div class=main-header > <a id=github  href="https://github.com/storopoli/Bayesian-Julia">Code on GitHub</a> </div> <div class=franklin-content ><div class=franklin-toc ><ol><li><a href="#probabilistic_programming">Probabilistic Programming</a><li><a href="#turings_ecosystem">Turing&#39;s Ecosystem</a><li><a href="#turingjl"><code>Turing.jl</code></a><ol><li><a href="#simulating_data">Simulating Data</a><li><a href="#visualizations">Visualizations</a></ol><li><a href="#prior_and_posterior_predictive_checks">Prior and Posterior Predictive Checks</a><li><a href="#conclusion">Conclusion</a><li><a href="#footnotes">Footnotes</a><li><a href="#references">References</a></ol></div> <h1 id=how_to_use_turing ><a href="#how_to_use_turing" class=header-anchor >How to use Turing</a></h1> <p><a href="http://turing.ml/"><strong>Turing</strong></a> is an ecosystem of Julia packages for Bayesian Inference using <a href="https://en.wikipedia.org/wiki/Probabilistic_programming">probabilistic programming</a>. Turing provides an easy and intuitive way of specifying models.</p> <h2 id=probabilistic_programming ><a href="#probabilistic_programming" class=header-anchor >Probabilistic Programming</a></h2> <p>What is <strong>probabilistic programming</strong> &#40;PP&#41;? It is a <strong>programming paradigm</strong> in which probabilistic models are specified and inference for these models is performed <strong>automatically</strong> &#40;Hardesty, 2015&#41;. In more clear terms, PP and PP Languages &#40;PPLs&#41; allows us to specify <strong>variables as random variables</strong> &#40;like Normal, Binominal etc.&#41; with <strong>known or unknown parameters</strong>. Then, we <strong>construct a model</strong> using these variables by specifying how the variables related to each other, and finally <strong>automatic inference of the variables&#39; unknown parameters</strong> is then performed.</p> <p>In a Bayesian approach this means specifying <strong>priors</strong>, <strong>likelihoods</strong> and letting the PPL compute the <strong>posterior</strong>. Since the denominator in the posterior is often intractable, we use Markov Chain Monte Carlo<sup id="fnref:MCMC"><a href="#fndef:MCMC" class=fnref >[1]</a></sup> and some fancy algorithm that uses the posterior geometry to guide the MCMC proposal using Hamiltonian dynamics called Hamiltonian Monte Carlo &#40;HMC&#41; to approximate the posterior. This involves, besides a suitable PPL, automatic differentiation, MCMC chains interface, and also an efficient HMC algorithm implementation. In order to provide all of these features, Turing has a whole ecosystem to address each and every one of these components.</p> <h2 id=turings_ecosystem ><a href="#turings_ecosystem" class=header-anchor >Turing&#39;s Ecosystem</a></h2> <p>Before we dive into how to specify models in Turing, let&#39;s discuss Turing&#39;s <strong>ecosystem</strong>. We have several Julia packages under Turing&#39;s GitHub organization <a href="https://github.com/TuringLang">TuringLang</a>, but I will focus on 6 of those:</p> <ul> <li><p><a href="https://github.com/TuringLang/Turing.jl"><code>Turing.jl</code></a></p> <li><p><a href="https://github.com/TuringLang/MCMCChains.jl"><code>MCMCChains.jl</code></a></p> <li><p><a href="https://github.com/TuringLang/DynamicPPL.jl"><code>DynamicPPL.jl</code></a></p> <li><p><a href="https://github.com/TuringLang/AdvancedHMC.jl"><code>AdvancedHMC.jl</code></a></p> <li><p><a href="https://github.com/TuringLang/DistributionsAD.jl"><code>DistributionsAD.jl</code></a></p> <li><p><a href="https://github.com/TuringLang/Bijectors.jl"><code>Bijectors.jl</code></a></p> </ul> <p>The first one is <a href="https://github.com/TuringLang/Turing.jl"><code>Turing.jl</code></a> &#40;Ge, Xu &amp; Ghahramani, 2018&#41; itself, the main package that we use to <strong>interface with all the Turing ecosystem</strong> of packages and the backbone of the PPL Turing.</p> <p>The second, <a href="https://github.com/TuringLang/MCMCChains.jl"><code>MCMCChains.jl</code></a>, is an interface to <strong>summarizing MCMC simulations</strong> and has several utility functions for <strong>diagnostics</strong> and <strong>visualizations</strong>.</p> <p>The third package is <a href="https://github.com/TuringLang/DynamicPPL.jl"><code>DynamicPPL.jl</code></a> &#40;Tarek, Xu, Trapp, Ge &amp; Ghahramani, 2020&#41; which specifies a domain-specific language and backend for Turing &#40;which itself is a PPL&#41;. The main feature of <code>DynamicPPL.jl</code> is that is is entirely written in Julia and also it is modular.</p> <p><a href="https://github.com/TuringLang/AdvancedHMC.jl"><code>AdvancedHMC.jl</code></a> &#40;Xu, Ge, Tebbutt, Tarek, Trapp &amp; Ghahramani, 2020&#41; provides a robust, modular and efficient implementation of advanced HMC algorithms. The state-of-the-art HMC algorithm is the <strong>N</strong>o-<strong>U</strong>-<strong>T</strong>urn <strong>S</strong>ampling &#40;NUTS&#41;<sup id="fnref:MCMC"><a href="#fndef:MCMC" class=fnref >[1]</a></sup> &#40;Hoffman &amp; Gelman, 2011&#41; which is available in <code>AdvancedHMC.jl</code>.</p> <p>The fourth package, <a href="https://github.com/TuringLang/DistributionsAD.jl"><code>DistributionsAD.jl</code></a> defines the necessary functions to enable automatic differentiation &#40;AD&#41; of the <code>logpdf</code> function from <a href="https://github.com/JuliaStats/Distributions.jl"><code>Distributions.jl</code></a> using the packages <a href="https://github.com/FluxML/Tracker.jl"><code>Tracker.jl</code></a>, <a href="https://github.com/FluxML/Zygote.jl"><code>Zygote.jl</code></a>, <a href="https://github.com/JuliaDiff/ForwardDiff.jl"><code>ForwardDiff.jl</code></a> and <a href="https://github.com/JuliaDiff/ReverseDiff.jl"><code>ReverseDiff.jl</code></a>. The main goal of <code>DistributionsAD.jl</code> is to make the output of <code>logpdf</code> differentiable with respect to all continuous parameters of a distribution as well as the random variable in the case of continuous distributions. This is the package that guarantees the &quot;automatic inference&quot; part of the definition of a PPL.</p> <p>Finally, <a href="https://github.com/TuringLang/Bijectors.jl"><code>Bijectors.jl</code></a> implements a set of functions for transforming constrained random variables &#40;e.g. simplexes, intervals&#41; to Euclidean space. Note that <code>Bijectors.jl</code> is still a work-in-progress and in the future we&#39;ll have better implementation for more constraints, <em>e.g.</em> positive ordered vectors of random variables.</p> <p>Most of the time we will not be dealing with these packages directly, since <code>Turing.jl</code> will take care of the interfacing for us. So let&#39;s talk about <code>Turing.jl</code>.</p> <h2 id=turingjl ><a href="#turingjl" class=header-anchor ><code>Turing.jl</code></a></h2> <p><code>Turing.jl</code> is the main package in the Turing ecosystem and the backbone that glues all the other packages together. Turing&#39;s &quot;workflow&quot; begin with a model specification. We specify the model inside a macro <code>@model</code> where we can assign variables in two ways:</p> <ul> <li><p>using <code>~</code>: which means that a variable follows some probability distribution &#40;Normal, Binomial etc.&#41; and its value is random under that distribution</p> <li><p>using <code>&#61;</code>: which means that a variable does not follow a probability distribution and its value is deterministic &#40;like the normal <code>&#61;</code> assignment in programming languages&#41;</p> </ul> <p>Turing will perform automatic inference on all variables that you specify using <code>~</code>. Here is a simple example of how we would model a six-sided dice. Note that a &quot;fair&quot; dice will be distributed as a discrete uniform probability with the lower bound as 1 and the upper bound as 6:</p> <a id=uniformdice  class=anchor ></a><span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>X</mi><mo>∼</mo><mtext>Uniform</mtext><mo stretchy=false >(</mo><mn>1</mn><mo separator=true >,</mo><mn>6</mn><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> X \sim \text{Uniform}(1,6) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >∼</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class=mord >Uniform</span></span><span class=mopen >(</span><span class=mord >1</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord >6</span><span class=mclose >)</span></span></span></span></span> <p>Note that the expectation of a random variable <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>∼</mo><mtext>Uniform</mtext><mo stretchy=false >(</mo><mi>a</mi><mo separator=true >,</mo><mi>b</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">X \sim \text{Uniform}(a,b)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >∼</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class=mord >Uniform</span></span><span class=mopen >(</span><span class="mord mathnormal">a</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">b</span><span class=mclose >)</span></span></span></span> is:</p> <a id=expectationdice  class=anchor ></a><span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>E</mi><mo stretchy=false >(</mo><mi>X</mi><mo stretchy=false >)</mo><mo>=</mo><mfrac><mrow><mi>a</mi><mo>+</mo><mi>b</mi></mrow><mn>2</mn></mfrac><mo>=</mo><mfrac><mn>7</mn><mn>2</mn></mfrac><mo>=</mo><mn>3.5</mn></mrow><annotation encoding="application/x-tex"> E(X) = \frac{a+b}{2} = \frac{7}{2} = 3.5 </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:2.05744em;vertical-align:-0.686em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.37144em;"><span style="top:-2.314em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >2</span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class="mord mathnormal">a</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">b</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:2.00744em;vertical-align:-0.686em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.32144em;"><span style="top:-2.314em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >2</span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >7</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.64444em;vertical-align:0em;"></span><span class=mord >3.5</span></span></span></span></span> <p>Graphically this means:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> Plots, StatsPlots, Distributions, LaTeXStrings

dice = DiscreteUniform(<span class=hljs-number >1</span>, <span class=hljs-number >6</span>)
plot(dice,
    label=<span class=hljs-string >&quot;six-sided Dice&quot;</span>,
    markershape=:circle,
    ms=<span class=hljs-number >5</span>,
    xlabel=<span class=hljs-string >L&quot;\theta&quot;</span>,
    ylabel=<span class=hljs-string >&quot;Mass&quot;</span>,
    ylims=(<span class=hljs-number >0</span>, <span class=hljs-number >0.3</span>)
)
vline!([mean(dice)], lw=<span class=hljs-number >5</span>, col=:red, label=<span class=hljs-string >L&quot;E(\theta)&quot;</span>)</code></pre> <p><img src="/Bayesian-Julia/assets/pages/4_Turing/code/output/dice.svg" alt=""> <div class=text-center ><em>A &quot;fair&quot; six-sided Dice: Discrete Uniform between 1 and 6</em></div> <br/></p> <p>So let&#39;s specify our first Turing model. It will be named <code>dice_throw</code> and will have a single parameter <code>y</code> which is a <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>-dimensional vector of integers representing the observed data, <em>i.e.</em> the outcomes of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> six-sided dice throws:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> Turing

<span class=hljs-meta >@model</span> <span class=hljs-keyword >function</span> dice_throw(y)
    <span class=hljs-comment >#Our prior belief about the probability of each result in a six-sided dice.</span>
    <span class=hljs-comment >#p is a vector of length 6 each with probability p that sums up to 1.</span>
    p ~ Dirichlet(<span class=hljs-number >6</span>, <span class=hljs-number >1</span>)

    <span class=hljs-comment >#Each outcome of the six-sided dice has a probability p.</span>
    y ~ filldist(Categorical(p), length(y))
<span class=hljs-keyword >end</span>;</code></pre> <p>Here we are using the <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distribution</a> which is the multivariate generalization of the <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>. The Dirichlet distribution is often used as the conjugate prior for Categorical or Multinomial distributions. Our dice is modelled as a <a href="https://en.wikipedia.org/wiki/Categorical_distribution">Categorical distribution</a> with six possible results <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>∈</mo><mo stretchy=false >{</mo><mn>1</mn><mo separator=true >,</mo><mn>2</mn><mo separator=true >,</mo><mn>3</mn><mo separator=true >,</mo><mn>4</mn><mo separator=true >,</mo><mn>5</mn><mo separator=true >,</mo><mn>6</mn><mo stretchy=false >}</mo></mrow><annotation encoding="application/x-tex">y \in \{ 1, 2, 3, 4, 5, 6 \}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.7335400000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >∈</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >{</span><span class=mord >1</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord >2</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord >3</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord >4</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord >5</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord >6</span><span class=mclose >}</span></span></span></span> with some probability vector <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold >p</mi><mo>=</mo><mo stretchy=false >(</mo><msub><mi>p</mi><mn>1</mn></msub><mo separator=true >,</mo><mo>…</mo><mo separator=true >,</mo><msub><mi>p</mi><mn>6</mn></msub><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">\mathbf{p} = (p_1, \dots, p_6)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.63888em;vertical-align:-0.19444em;"></span><span class="mord mathbf">p</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=minner >…</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span>. Since all mutually exclusive outcomes must sum up to 1 to be a valid probability, we impose the constraint that all <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span></span></span></span>s must sum up to 1 – <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sum^n_{i=1} p_i = 1</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.104002em;vertical-align:-0.29971000000000003em;"></span><span class=mop ><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord ><span class="mord mathnormal">p</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.64444em;vertical-align:0em;"></span><span class=mord >1</span></span></span></span>. We could have used a vector of six Beta random variables but it would be hard and inefficient to enforce this constraint. Instead, I&#39;ve opted for a Dirichlet with a weekly informative prior towards a &quot;fair&quot; dice which is encoded as a <code>Dirichlet&#40;6,1&#41;</code>. This is translated as a 6-dimensional vector of elements that sum to one:</p> <pre><code class="julia hljs">mean(Dirichlet(<span class=hljs-number >6</span>, <span class=hljs-number >1</span>))</code></pre><pre><code class="plaintext hljs">6-element Fill{Float64}, with entries equal to 0.16666666666666666</code></pre>
<p>And, indeed, it sums up to one:</p>
<pre><code class="julia hljs">sum(mean(Dirichlet(<span class=hljs-number >6</span>, <span class=hljs-number >1</span>)))</code></pre><pre><code class="plaintext hljs">1.0</code></pre>
<p>Also, since the outcome of a <a href="https://en.wikipedia.org/wiki/Categorical_distribution">Categorical distribution</a> is an integer and <code>y</code> is a <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>-dimensional vector of integers we need to apply some sort of broadcasting here. <code>filldist&#40;&#41;</code> is a nice Turing function which takes any univariate or multivariate distribution and returns another distribution that repeats the input distribution. We could also use the familiar dot <code>.</code> broadcasting operator in Julia: <code>y .~ Categorical&#40;p&#41;</code> to signal that all elements of <code>y</code> are distributed as a Categorical distribution. But doing that does not allow us to do predictive checks &#40;more on this below&#41;. So, instead we use <code>filldist&#40;&#41;</code>.</p>
<h3 id=simulating_data ><a href="#simulating_data" class=header-anchor >Simulating Data</a></h3>
<p>Now let&#39;s set a seed for the pseudo-random number generator and simulate 1,000 throws of a six-sided dice:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> Random

Random.seed!(<span class=hljs-number >123</span>);

data = rand(DiscreteUniform(<span class=hljs-number >1</span>, <span class=hljs-number >6</span>), <span class=hljs-number >1_000</span>);</code></pre>
<p>The vector <code>data</code> is a 1,000-length vector of <code>Int</code>s ranging from 1 to 6, just like how a regular six-sided dice outcome would be:</p>
<pre><code class="julia hljs">first(data, <span class=hljs-number >5</span>)</code></pre><pre><code class="plaintext hljs">5-element Vector{Int64}:
 4
 4
 6
 2
 4</code></pre>
<p>Once the model is specified we instantiate the model with the single parameter <code>y</code> as the simulated <code>data</code>:</p>
<pre><code class="julia hljs">model = dice_throw(data);</code></pre>
<p>Next, we call Turing&#39;s <code>sample&#40;&#41;</code> function that takes a Turing model as a first argument, along with a sampler as the second argument, and the third argument is the number of iterations. Here, I will use the <code>NUTS&#40;&#41;</code> sampler from <code>AdvancedHMC.jl</code> and 2,000 iterations. Please note that, as default, Turing samplers will discard the first thousand &#40;1,000&#41; iterations as warmup. So the sampler will output 2,000 samples starting from sample 1,001 until sample 3,000:</p>
<pre><code class="julia hljs">chain = sample(model, NUTS(), <span class=hljs-number >2_000</span>);</code></pre>
<p>Now let&#39;s inspect the chain. We can do that with the function <code>describe&#40;&#41;</code> that will return a 2-element vector of <code>ChainDataFrame</code> &#40;this is the type defined by <code>MCMCChains.jl</code> to store Markov chain&#39;s information regarding the inferred parameters&#41;. The first <code>ChainDataFrame</code> has information regarding the parameters&#39; summary statistics &#40;<code>mean</code>, <code>std</code>, <code>r_hat</code>, ...&#41; and the second is the parameters&#39; quantiles. Since <code>describe&#40;chain&#41;</code> returns a 2-element vector, I will assign the output to two variables:</p>
<pre><code class="julia hljs">summaries, quantiles = describe(chain);</code></pre>
<p>We won&#39;t be focusing on quantiles, so let&#39;s put it aside for now. Let&#39;s then take a look at the parameters&#39; summary statistics:</p>
<pre><code class="julia hljs">summaries</code></pre><pre><code class="plaintext hljs">Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64

        p[1]    0.1573    0.0114     0.0003    0.0002   3272.7144    0.9995      117.6009
        p[2]    0.1450    0.0114     0.0003    0.0002   3254.4667    0.9996      116.9452
        p[3]    0.1402    0.0110     0.0002    0.0002   2794.0041    0.9998      100.3990
        p[4]    0.1812    0.0121     0.0003    0.0002   2867.9490    1.0003      103.0561
        p[5]    0.1965    0.0125     0.0003    0.0002   2861.1111    0.9998      102.8104
        p[6]    0.1799    0.0122     0.0003    0.0002   2756.1952    0.9998       99.0404
</code></pre>
<p>Here <code>p</code> is a 6-dimensional vector of probabilities, which each one associated with a mutually exclusive outcome of a six-sided dice throw. As we expected, the probabilities are almost equal to <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>6</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{6}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.190108em;vertical-align:-0.345em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">6</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>, like a &quot;fair&quot; six-sided dice that we simulated data from &#40;sampling from <code>DiscreteUniform&#40;1, 6&#41;</code>&#41;. Indeed, just for a sanity check, the mean of the estimates of <code>p</code> sums up to 1:</p>
<pre><code class="julia hljs">sum(summaries[:, :mean])</code></pre><pre><code class="plaintext hljs">1.0</code></pre>
<p>In the future if you have some crazy huge models and you just want a <strong>subset</strong> of parameters from your chains? Just do <code>group&#40;chain, :parameter&#41;</code> or index with <code>chain&#91;:, 1:6, :&#93;</code>:</p>
<pre><code class="julia hljs">summarystats(chain[:, <span class=hljs-number >1</span>:<span class=hljs-number >3</span>, :])</code></pre><pre><code class="plaintext hljs">Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64

        p[1]    0.1573    0.0114     0.0003    0.0002   3272.7144    0.9995      117.6009
        p[2]    0.1450    0.0114     0.0003    0.0002   3254.4667    0.9996      116.9452
        p[3]    0.1402    0.0110     0.0002    0.0002   2794.0041    0.9998      100.3990
</code></pre>
<p>or <code>chain&#91;&#91;:parameters,...&#93;&#93;</code>:</p>
<pre><code class="julia hljs">summarystats(chain[[:<span class=hljs-string >var&quot;p[1]&quot;</span>, :<span class=hljs-string >var&quot;p[2]&quot;</span>]])</code></pre><pre><code class="plaintext hljs">Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64

        p[1]    0.1573    0.0114     0.0003    0.0002   3272.7144    0.9995      117.6009
        p[2]    0.1450    0.0114     0.0003    0.0002   3254.4667    0.9996      116.9452
</code></pre>
<p>And, finally let&#39;s compute the expectation of the estimated six-sided dice, <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy=false >(</mo><mover accent=true ><mi>X</mi><mo>~</mo></mover><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">E(\tilde{X})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.1701899999999998em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class=mopen >(</span><span class="mord accent"><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.9201899999999998em;"><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span><span style="top:-3.6023300000000003em;"><span class=pstrut  style="height:3em;"></span><span class=accent-body  style="left:-0.16666em;"><span class=mord >~</span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span>, using the standard expectation definition of expectation for a discrete random variable:</p>
<span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>E</mi><mo stretchy=false >(</mo><mi>X</mi><mo stretchy=false >)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></munder><mi>x</mi><mo>⋅</mo><mi>P</mi><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> E(X) = \sum_{x \in X} x \cdot P(x) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:2.3717110000000003em;vertical-align:-1.321706em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.050005em;"><span style="top:-1.8556639999999998em;margin-left:0em;"><span class=pstrut  style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span style="top:-3.0500049999999996em;"><span class=pstrut  style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:1.321706em;"><span></span></span></span></span></span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">x</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >⋅</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span></span>
<pre><code class="julia hljs">sum([idx * i <span class=hljs-keyword >for</span> (i, idx) <span class=hljs-keyword >in</span> enumerate(summaries[:, :mean])])</code></pre><pre><code class="plaintext hljs">3.6543206550249376</code></pre>
<p>Bingo&#33; The estimated expectation is very <em>close</em> to the theoretical expectation of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>7</mn><mn>2</mn></mfrac><mo>=</mo><mn>3.5</mn></mrow><annotation encoding="application/x-tex">\frac{7}{2} = 3.5</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.190108em;vertical-align:-0.345em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">7</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.64444em;vertical-align:0em;"></span><span class=mord >3.5</span></span></span></span>, as we&#39;ve show in <span class=eqref >(<a href="#expectationdice">2</a>)</span>.</p>
<h3 id=visualizations ><a href="#visualizations" class=header-anchor >Visualizations</a></h3>
<p>Note that the type of our <code>chain</code> is a <code>Chains</code> object from <code>MCMCChains.jl</code>:</p>
<pre><code class="julia hljs">typeof(chain)</code></pre><pre><code class="plaintext hljs">MCMCChains.Chains{Float64, AxisArrays.AxisArray{Float64, 3, Array{Float64, 3}, Tuple{AxisArrays.Axis{:iter, StepRange{Int64, Int64}}, AxisArrays.Axis{:var, Vector{Symbol}}, AxisArrays.Axis{:chain, UnitRange{Int64}}}}, Missing, NamedTuple{(:parameters, :internals), Tuple{Vector{Symbol}, Vector{Symbol}}}, NamedTuple{(:start_time, :stop_time), Tuple{Float64, Float64}}}</code></pre>
<p>We can use plotting capabilities of <code>MCMCChains.jl</code> with any <code>Chains</code> object:</p>
<pre><code class="julia hljs">plot(chain)</code></pre>
<p><img src="/Bayesian-Julia/assets/pages/4_Turing/code/output/chain.svg" alt=""> <div class=text-center ><em>Visualization of a MCMC Chain simulation</em></div> <br/></p>
<p>On the figure above we can see, for each parameter in the model, on the left the parameter&#39;s traceplot and on the right the parameter&#39;s density<sup id="fnref:visualization"><a href="#fndef:visualization" class=fnref >[2]</a></sup>.</p>
<h2 id=prior_and_posterior_predictive_checks ><a href="#prior_and_posterior_predictive_checks" class=header-anchor >Prior and Posterior Predictive Checks</a></h2>
<p>Predictive checks are a great way to <strong>validate a model</strong>. The idea is to <strong>generate data from the model</strong> using <strong>parameters from draws from the prior or posterior</strong>. <strong>Prior predictive check</strong> is when we simulate data using model parameter values drawn fom the <strong>prior</strong> distribution, and <strong>posterior predictive check</strong> is is when we simulate data using model parameter values drawn fom the <strong>posterior</strong> distribution.</p>
<p>The workflow we do when specifying and sampling Bayesian models is not linear or acyclic &#40;Gelman et al., 2020&#41;. This means that we need to iterate several times between the different stages in order to find a model that captures best the data generating process with the desired assumptions. The figure below demonstrates the workflow <sup id="fnref:workflow"><a href="#fndef:workflow" class=fnref >[3]</a></sup>.</p>
<p><img src="/Bayesian-Julia/pages/images/bayesian_workflow.png" alt="Bayesian Workflow" /></p>
<p><div class=text-center ><em>Bayesian Workflow. Adapted from Gelman et al. &#40;2020&#41;</em></div> <br/></p>
<p>This is quite easy in Turing. Our six-sided dice model already has a <strong>posterior distribution</strong> which is the object <code>chain</code>. We need to create a <strong>prior distribution</strong> for our model. To accomplish this, instead of supplying a MCMC sampler like <code>NUTS&#40;&#41;</code>, we supply the &quot;sampler&quot; <code>Prior&#40;&#41;</code> inside Turing&#39;s <code>sample&#40;&#41;</code> function:</p>
<pre><code class="julia hljs">prior_chain = sample(model, Prior(), <span class=hljs-number >2_000</span>);</code></pre>
<p>Now we can perform predictive checks using both the prior &#40;<code>prior_chain</code>&#41; or posterior &#40;<code>chain</code>&#41; distributions. To draw from the prior and posterior predictive distributions we instantiate a &quot;predictive model&quot;, <em>i.e.</em> a Turing model but with the observations set to <code>missing</code><sup id="fnref:missing"><a href="#fndef:missing" class=fnref >[4]</a></sup>, and then calling <code>predict&#40;&#41;</code> on the predictive model and the previously drawn samples. First let&#39;s do the <em>prior</em> predictive check:</p>
<pre><code class="julia hljs">missing_data = <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Missing</span>}(<span class=hljs-literal >missing</span>, length(data)) <span class=hljs-comment ># vector of `missing`</span>
model_missing = dice_throw(missing_data)
model_predict = DynamicPPL.Model{(:y,)}(:model_predict_missing_data,
                    model_missing.f,
                    model_missing.args,
                    model_missing.defaults) <span class=hljs-comment ># instantiate the &quot;predictive model&quot;</span>
prior_check = predict(model_predict, prior_chain);</code></pre>
<p>Here we are creating a <code>missing_data</code> object which is a <code>Vector</code> of the same length as the <code>data</code> and populated with type <code>missing</code> as values. We then instantiate a new <code>dice_throw</code> model with the <code>missing_data</code> vector as the <code>data</code> argument. We proceed by instantiating a new Turing <code>DynamicPPL.Model</code> model with the <code>missing_data</code> vector as the <code>data</code> argument. The boilerplate around <code>DynamicPPL.Model</code> is the default arguments that a <code>DynamicPPL.Model</code> model needs to have. Finally, we call <code>predict&#40;&#41;</code> on the predictive model and the previously drawn samples, which in our case are the samples from the prior distribution &#40;<code>prior_chain</code>&#41;.</p>
<p>Note that <code>predict&#40;&#41;</code> returns a <code>Chains</code> object from <code>MCMCChains.jl</code>:</p>
<pre><code class="julia hljs">typeof(prior_check)</code></pre><pre><code class="plaintext hljs">MCMCChains.Chains{Float64, AxisArrays.AxisArray{Float64, 3, Array{Float64, 3}, Tuple{AxisArrays.Axis{:iter, StepRange{Int64, Int64}}, AxisArrays.Axis{:var, Vector{Symbol}}, AxisArrays.Axis{:chain, UnitRange{Int64}}}}, Missing, NamedTuple{(:parameters, :internals), Tuple{Vector{Symbol}, Vector{Symbol}}}, NamedTuple{(), Tuple{}}}</code></pre>
<p>And we can call <code>summarystats&#40;&#41;</code>:</p>
<pre><code class="julia hljs">summarystats(prior_check[:, <span class=hljs-number >1</span>:<span class=hljs-number >5</span>, :]) <span class=hljs-comment ># just the first 5 prior samples</span></code></pre><pre><code class="plaintext hljs">Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64

        y[1]    3.5130    1.6850     0.0377    0.0392   2208.1913    0.9995
        y[2]    3.5200    1.7240     0.0385    0.0339   1967.6621    1.0001
        y[3]    3.4360    1.7057     0.0381    0.0395   1938.0449    0.9997
        y[4]    3.5730    1.6962     0.0379    0.0341   2240.0070    0.9997
        y[5]    3.5075    1.7244     0.0386    0.0325   1973.6389    0.9997
</code></pre>
<p>We can do the same with <code>chain</code> for a <em>posterior</em> predictive check:</p>
<pre><code class="julia hljs">posterior_check = predict(model_predict, chain);
summarystats(posterior_check[:, <span class=hljs-number >1</span>:<span class=hljs-number >5</span>, :]) <span class=hljs-comment ># just the first 5 posterior samples</span></code></pre><pre><code class="plaintext hljs">Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64

        y[1]    3.6295    1.7031     0.0381    0.0382   1890.7910    0.9995
        y[2]    3.7100    1.7124     0.0383    0.0384   1844.6380    0.9997
        y[3]    3.6355    1.7135     0.0383    0.0372   1982.8912    1.0001
        y[4]    3.6985    1.7035     0.0381    0.0333   2122.5496    0.9996
        y[5]    3.5780    1.7382     0.0389    0.0457   2035.0654    0.9995
</code></pre>
<h2 id=conclusion ><a href="#conclusion" class=header-anchor >Conclusion</a></h2>
<p>This is the basic overview of Turing usage. I hope that I could show you how simple and intuitive is to specify probabilistic models using Turing. First, specify a <strong>model</strong> with the macro <code>@model</code>, then <strong>sample from it</strong> by specifying the <strong>data</strong>, <strong>sampler</strong> and <strong>number of interactions</strong>. All <strong>probabilistic parameters</strong> &#40;the ones that you&#39;ve specified using <code>~</code>&#41; will be <strong>inferred</strong> with a full <strong>posterior density</strong>. Finally, you inspect the <strong>parameters&#39; statistics</strong> like <strong>mean</strong> and <strong>standard deviation</strong>, along with <strong>convergence diagnostics</strong> like <code>r_hat</code>. Conveniently, you can <strong>plot</strong> stuff easily if you want to. You can also do <strong>predictive checks</strong> using either the <strong>posterior</strong> or <strong>prior</strong> model&#39;s distributions.</p>
<h2 id=footnotes ><a href="#footnotes" class=header-anchor >Footnotes</a></h2>
<p><table class=fndef  id="fndef:MCMC">
    <tr>
        <td class=fndef-backref ><a href="#fnref:MCMC">[1]</a>
        <td class=fndef-content >see <a href="/Bayesian-Julia/pages/5_MCMC/">5. <strong>Markov Chain Monte Carlo &#40;MCMC&#41;</strong></a>.
    
</table>
<table class=fndef  id="fndef:visualization">
    <tr>
        <td class=fndef-backref ><a href="#fnref:visualization">[2]</a>
        <td class=fndef-content >we&#39;ll cover those plots and diagnostics in <a href="/Bayesian-Julia/pages/5_MCMC/">5. <strong>Markov Chain Monte Carlo &#40;MCMC&#41;</strong></a>.
    
</table>
<table class=fndef  id="fndef:workflow">
    <tr>
        <td class=fndef-backref ><a href="#fnref:workflow">[3]</a>
        <td class=fndef-content >note that this workflow is a extremely simplified adaptation from the original workflow on which it was based. I suggest the reader to consult the original workflow of Gelman et al. &#40;2020&#41;.
    
</table>
<table class=fndef  id="fndef:missing">
    <tr>
        <td class=fndef-backref ><a href="#fnref:missing">[4]</a>
        <td class=fndef-content >in a real-world scenario, you&#39;ll probably want to use more than just <strong>one</strong> observation as a predictive check, so you should use something like <code>Vector&#123;Missing&#125;&#40;missing, length&#40;y&#41;&#41;</code> or <code>fill&#40;missing, length&#40;y&#41;</code>.
    
</table>
</p>
<h2 id=references ><a href="#references" class=header-anchor >References</a></h2>
<p>Ge, H., Xu, K., &amp; Ghahramani, Z. &#40;2018&#41;. Turing: A Language for Flexible Probabilistic Inference. International Conference on Artificial Intelligence and Statistics, 1682–1690. http://proceedings.mlr.press/v84/ge18b.html</p>
<p>Gelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., … Modr’ak, M. &#40;2020, November 3&#41;. Bayesian Workflow. Retrieved February 4, 2021, from http://arxiv.org/abs/2011.01808</p>
<p>Hardesty &#40;2015&#41;.  &quot;Probabilistic programming does in 50 lines of code what used to take thousands&quot;. phys.org. April 13, 2015. Retrieved April 13, 2015. https://phys.org/news/2015-04-probabilistic-lines-code-thousands.html</p>
<p>Hoffman, M. D., &amp; Gelman, A. &#40;2011&#41;. The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15&#40;1&#41;, 1593–1623. Retrieved from http://arxiv.org/abs/1111.4246</p>
<p>Tarek, M., Xu, K., Trapp, M., Ge, H., &amp; Ghahramani, Z. &#40;2020&#41;. DynamicPPL: Stan-like Speed for Dynamic Probabilistic Models. ArXiv:2002.02702 &#91;Cs, Stat&#93;. http://arxiv.org/abs/2002.02702</p>
<p>Xu, K., Ge, H., Tebbutt, W., Tarek, M., Trapp, M., &amp; Ghahramani, Z. &#40;2020&#41;. AdvancedHMC.jl: A robust, modular and efficient implementation of advanced HMC algorithms. Symposium on Advances in Approximate Bayesian Inference, 1–10. http://proceedings.mlr.press/v118/xu20a.html</p>

<div class=page-foot >
  <div class=copyright >
    Last modified: July 12, 2022. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div>
    </div> 
    </div> 
    </div> <!-- end of class page-wrap-->